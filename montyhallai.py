# -*- coding: utf-8 -*-
"""MontyHallAI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YfReuWCwls0nxsX0kB7ZWadbL_Apqx_L
"""

import random
door_option = ["Car", "Goat", "Goat"]
door_options = np.array(door_option)
door_letter = ["A", "B", "C"]
door_letters = np.array(door_letter)
random.shuffle(door_options)
random.shuffle(door_letter)

import numpy as np
def check_win(letter, door_letter, door_options):
  index = np.where(door_letter == letter)
  if door_options[index] == "Car":
    return True
  return False

def run_random_switch_round(choice):
  random_door = random.choice(door_letter)
  selected_index = door_letter.index(random_door)
  remaining_doors = door_letter[:selected_index] + door_letter[selected_index + 1:]
  for index, door in enumerate(remaining_doors):
    if door_option[index] == "Goat":
      pick_door = door
      break
  if (choice == 1):
    return check_win(random_door, door_letters, door_options)
  for index, door in enumerate(remaining_doors):
    if door != random_door and door != pick_door:
      return check_win(door, door_letters, door_options)
      break

run_random_switch_round(2)

outcomes = []
true_count = 0
false_count = 0

for i in range(200000):
    result = run_random_switch_round(1)
    outcomes.append(result)

    if result:
        true_count += 1
    else:
        false_count += 1

print("True count:", true_count)
print("False count:", false_count)

pip install matplotlib

import numpy as np
import matplotlib.pyplot as plt

num_actions = 2
num_states = 1

q_table = np.zeros((num_states, num_actions))

learning_rate = 0.1
discount_factor = 0.9
exploration_prob = 0.3

num_episodes = 100

success_rate_over_time = []
total_true_outcomes = 0

computer_decisions = []

for episode in range(num_episodes):
    if (episode % (num_episodes/10) == 0):
      perc_done = int(episode/num_episodes * 100)
      print(str(perc_done) + "% done")
    if (episode == (num_episodes/2)):
      exploration_prob = 0

    state = 0
    if np.random.uniform(0, 1) < exploration_prob:
        action = np.random.choice(num_actions)
    else:
        action = np.argmax(q_table[state, :])

    outcome = run_random_switch_round(action)
    computer_decisions.append(action)

    next_state = state
    if outcome:
        reward = 1
        total_true_outcomes += 1

    q_table[state, action] = q_table[state, action] + learning_rate * (reward + discount_factor * np.max(q_table[next_state, :]) - q_table[state, action])

    state = next_state


    success_rate = total_true_outcomes / (episode + 1)
    success_rate_over_time.append(success_rate)

print("Q-Table after training:")
print(q_table)

plt.plot(success_rate_over_time)
plt.title("Success Rate Over Time")
plt.xlabel("Episode")
plt.ylabel("Success Rate")
plt.show()

print("Computer's Decisions:")
print(computer_decisions)

unique_values, counts = np.unique(computer_decisions, return_counts=True)
for value, count in zip(unique_values, counts):
    print(f"{value}: {count}")

first_30_values = computer_decisions[:30]
unique_values, counts = np.unique(first_30_values, return_counts=True)
for value, count in zip(unique_values, counts):
    print(f"{value}: {count}")

middle_start = len(computer_decisions) // 2 - 15
middle_end = middle_start + 30
middle_30_values = computer_decisions[middle_start:middle_end]
unique_values, counts = np.unique(middle_30_values, return_counts=True)
for value, count in zip(unique_values, counts):
    print(f"{value}: {count}")

last_30_values = computer_decisions[-30:]
unique_values, counts = np.unique(last_30_values, return_counts=True)
for value, count in zip(unique_values, counts):
    print(f"{value}: {count}")

q_table = np.zeros((num_states, num_actions))
def choose_action():
    if np.random.uniform(0, 1) < exploration_prob:
        action = np.random.choice(num_actions)
    else:
        action = np.argmax(q_table[state, :])
    return action

choose_action()